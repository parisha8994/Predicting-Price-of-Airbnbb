# -*- coding: utf-8 -*-
"""Airbnb_prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1td3oZLEwg0Zq81CSC33Ja8gwEzcPM17S
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import OneHotEncoder
import plotly.express as px
#from sklearn.linear_model import LinearRegression
from sklearn.svm import SVR
from sklearn.preprocessing import StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score

# Load the dataset
df = pd.read_csv('Airbnb_Open_Data.csv')

# Drop irrelevant columns
df = df.drop(['id', 'host id','NAME' ,'host name', 'country', 'house_rules', 'license'], axis=1)

# Drop rows with missing values in specific columns
df = df.dropna(subset=['price', 'host_identity_verified'])
df = df.dropna(subset=['neighbourhood group', 'neighbourhood'])
df = df.dropna(subset=['cancellation_policy'])
df = df.dropna(subset=['lat', 'long'])

# Rename 'neighbourhood group' column
df.rename(columns={'neighbourhood group': 'neighbourhood_group'}, inplace=True)
df = df.dropna(subset=['neighbourhood_group'])

# Handle missing values
df['reviews per month'].fillna(0, inplace=True)
df['last review'].fillna('Unknown', inplace=True)
df.dropna(subset=['price'], inplace=True)


# Preprocess price and service fee columns
df['price'] = df['price'].astype(str).str.replace('$', '').str.replace(',', '').astype(float)
df['service fee'] = df['service fee'].astype(str).str.replace('$', '').str.replace(',', '').astype(float)

# Remove duplicates
df.drop_duplicates(inplace=True)

# Split into features (X) and target variable (y)
X = df.drop('price', axis=1)
y = df['price']

# Visualization and exploratory data analysis

# Distribution of Room Types
room_type_counts = df['room type'].value_counts()
room_type_counts.plot(kind='bar', xlabel='Room Type', ylabel='Count', title='Distribution of Room Types')
plt.show()

# Correlation Analysis
correlation_matrix = df[['price', 'reviews per month', 'availability 365']].corr()
print(correlation_matrix)

# Host Verification and Identity
verification_counts = df['host_identity_verified'].value_counts()
verification_counts.plot(kind='bar', xlabel='Host Verification', ylabel='Count', title='Host Verification Distribution')
plt.show()

# Reviews and Ratings
plt.scatter(df['number of reviews'], df['reviews per month'])
plt.xlabel('Number of Reviews')
plt.ylabel('Reviews per Month')
plt.title('Reviews Analysis')
plt.show()

# Host Listings Count
plt.hist(df['calculated host listings count'], bins=20)
plt.xlabel('Host Listings Count')
plt.ylabel('Count')
plt.title('Distribution of Host Listings Count')
plt.show()

# Filter and group by neighborhood
neighborhood_reviews = df[['neighbourhood', 'number of reviews']]
neighborhood_reviews = neighborhood_reviews.groupby('neighbourhood').sum()

# Sort by number of reviews
neighborhood_reviews = neighborhood_reviews.sort_values('number of reviews', ascending=False)

# Select top 25 neighborhoods
top_25_neighborhoods = neighborhood_reviews.head(25)

# Create a color palette
colors = sns.color_palette('Blues', len(top_25_neighborhoods))

# Create a bar chart with different colors
plt.figure(figsize=(12, 12))
top_25_neighborhoods['number of reviews'].plot(kind='bar', color=colors)
plt.xlabel('Neighborhood')
plt.ylabel('Number of Reviews')
plt.title('Top 25 Most Reviewed Neighborhoods')
plt.xticks(rotation=90)
plt.tight_layout()
plt.show()

#Listing Availability Distribution

plt.figure(figsize=(10, 7))
sns.histplot(data=df, x='availability 365', kde=True)
plt.xlim(0, 400)
plt.xlabel('Availability (in days)')
plt.ylabel('Count')
plt.title('Listing Availability Distribution')
plt.show()

#Distribution of Prices

fig, ax = plt.subplots(figsize=(15, 10))
sns.set(font_scale=2)
sns.histplot(data=df, x='price', color='orange')
plt.xlabel('Price')
plt.ylabel('Count')
plt.title('Distribution of Prices')
plt.xlim(0, 1200)
plt.show()

#Average price per construction year

price_per_year = df.groupby('Construction year')['price'].median()
fig = px.line(price_per_year, x=price_per_year.index, y=price_per_year.values,
labels={'x': 'Construction year', 'y': 'Average price'},
text=['$' + str(int(i)) for i in price_per_year.values],
title='Average price per construction year in USD',
color_discrete_sequence=px.colors.sequential.Teal_r,
template='plotly_dark')
fig.update_layout(font=dict(size=16, color='white', family='Avenir'))
fig.show()

#Average price per room type

price_per_room_type = df.groupby('room type')['price'].median()
fig = px.bar(price_per_room_type, x=price_per_room_type.index, y=price_per_room_type.values,
labels={'x': 'Room type', 'y': 'Average price'},
text=['$' + str(int(i)) for i in price_per_room_type.values],
title='Average price per room type in USD',
color_discrete_sequence=px.colors.sequential.Bluyl,
template='plotly_dark')
fig.update_layout(font=dict(size=16, color='white', family='Avenir'))
fig.show()

#Average price per construction year (bar chart)

fig = px.bar(price_per_year, x=price_per_year.index, y=price_per_year.values,
labels={'x': 'Construction year', 'y': 'Average price'},
text=['$' + str(int(i)) for i in price_per_year.values],
title='Average price per construction year in USD',
color_discrete_sequence=px.colors.sequential.RdBu)
fig.update_layout(font=dict(size=16, color='black', family='Avenir'))
fig.show()

#Encoding categorical variables

#label_encoder = LabelEncoder()
#df['host_identity_verified'] = label_encoder.fit_transform(df['host_identity_verified'])
#df['instant_bookable'] = label_encoder.fit_transform(df['instant_bookable'])
#df['cancellation_policy'] = label_encoder.fit_transform(df['cancellation_policy'])
#df['room type'] = label_encoder.fit_transform(df['room type'])
#df['neighbourhood_group'] = label_encoder.fit_transform(df['neighbourhood_group'])
#df['country code'] = label_encoder.fit_transform(df['country code'])
#df['neighbourhood'] = label_encoder.fit_transform(df['neighbourhood'])

# Convert 'last review' column to datetime type
df['last review'] = pd.to_datetime(df['last review'], errors='coerce')

# Handle 'Unknown' values in 'last review' column
df.loc[df['last review'].isnull(), 'last review'] = 'Unknown'

# Convert 'last review' column to datetime type again
df['last review'] = pd.to_datetime(df['last review'], errors='coerce')

#Select only object type columns

categorical_columns = df.select_dtypes('object').columns

df.select_dtypes('object')

df.dropna(inplace=True)

imputer = SimpleImputer(strategy='most_frequent')
X = imputer.fit_transform(X)

#label_encoder = LabelEncoder()
#X_encoded = X.copy()
#for column in categorical_columns:
 #   X_encoded[column] = label_encoder.fit_transform(X[column])

# Splitting the data into training and testing sets
#from sklearn.model_selection import train_test_split

#X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)

# Identify categorical columns
categorical_columns = ['room type', 'neighbourhood_group', 'cancellation_policy', 'country code', 'neighbourhood']

# Perform one-hot encoding
encoder = OneHotEncoder(drop='first', sparse=False)
encoded_columns = encoder.fit_transform(df[categorical_columns])
encoded_df = pd.DataFrame(encoded_columns, columns=encoder.get_feature_names_out(categorical_columns))

# Concatenate encoded columns with numerical columns
df_encoded = pd.concat([df.drop(categorical_columns, axis=1), encoded_df], axis=1)

# Select the remaining categorical columns
remaining_categorical_columns = df.select_dtypes('object').columns

# Encode the remaining categorical columns using pd.get_dummies
df_encoded = pd.get_dummies(df, columns=remaining_categorical_columns)

# Drop rows with missing values from the training data
X_train_encoded.dropna(inplace=True)
y_train_encoded = y_train_encoded[X_train_encoded.index]

# Split into features (X) and target variable (y)
X_encoded = df_encoded.drop('price', axis=1)
y_encoded = df_encoded['price']

# Split the encoded data into training and test sets
X_train_encoded, X_test_encoded, y_train_encoded, y_test_encoded = train_test_split(X_encoded, y_encoded, test_size=0.2, random_state=42)

# Initialize the imputer
imputer = SimpleImputer(strategy='mean')

# Impute the missing values in X_train_encoded and X_test_encoded
X_train_encoded_imputed = imputer.fit_transform(X_train_encoded)
X_test_encoded_imputed = imputer.transform(X_test_encoded)

# Initialize and fit the decision tree regression model on the imputed data
model_encoded = DecisionTreeRegressor()
model_encoded.fit(X_train_encoded_imputed, y_train_encoded)

y_pred_encoded = model_encoded.predict(X_test_encoded)

# Create an imputer object and fit it on the training data
imputer = SimpleImputer(strategy='mean')
imputer.fit(X_train_encoded)

# Impute missing values in the test data
X_test_encoded = imputer.transform(X_test_encoded)

# Make predictions on the test data
y_pred_encoded = model_encoded.predict(X_test_encoded)

# Calculate evaluation metrics
mse = mean_squared_error(y_test_encoded, y_pred_encoded)
rmse = np.sqrt(mse)
r2 = r2_score(y_test_encoded, y_pred_encoded)

# Print the evaluation metrics
print('Mean Squared Error:', mse)
print('Root Mean Squared Error:', rmse)
print('R-squared Score:', r2)